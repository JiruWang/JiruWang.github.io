<!DOCTYPE html>
<html lang="en">
<head><!-- hexo injector head_begin start -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\(","\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
    </script>
    <!-- hexo injector head_begin end -->
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jiruwang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="GRPOPolicy Gradient Trajectory Level Expectation  argmax(\pi_{\theta}) J_(\pi_{\theta)} &#x3D; E_{\tau\sim \pi_{\theta}}[R(\tau)] &#x3D; \sum_{\tau}R(\tau)P(\tau|\pi_{\theta}) \\ Derivate (REINFORCE: score func">
<meta property="og:type" content="article">
<meta property="og:title" content="GRPO in OpenRLHF">
<meta property="og:url" content="https://jiruwang.github.io/2024/10/30/OpenRLHF-PPO-GRPO/index.html">
<meta property="og:site_name" content="Jiru&#39;s Blog">
<meta property="og:description" content="GRPOPolicy Gradient Trajectory Level Expectation  argmax(\pi_{\theta}) J_(\pi_{\theta)} &#x3D; E_{\tau\sim \pi_{\theta}}[R(\tau)] &#x3D; \sum_{\tau}R(\tau)P(\tau|\pi_{\theta}) \\ Derivate (REINFORCE: score func">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-10-30T14:30:00.000Z">
<meta property="article:modified_time" content="2025-11-10T08:30:36.652Z">
<meta property="article:author" content="jiru">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jiruwang.github.io/2024/10/30/OpenRLHF-PPO-GRPO/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jiruwang.github.io/2024/10/30/OpenRLHF-PPO-GRPO/","path":"2024/10/30/OpenRLHF-PPO-GRPO/","title":"GRPO in OpenRLHF"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>GRPO in OpenRLHF | Jiru's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"single_dollars":true,"enable":true,"mhchem":true,"cdn":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML","tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Jiru's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GRPO"><span class="nav-number">1.</span> <span class="nav-text">GRPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Gradient"><span class="nav-number">1.1.</span> <span class="nav-text">Policy Gradient</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implention-of-OpenRLHF"><span class="nav-number">1.2.</span> <span class="nav-text">Implention of OpenRLHF</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jiru</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jiruwang.github.io/2024/10/30/OpenRLHF-PPO-GRPO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jiru">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiru's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="GRPO in OpenRLHF | Jiru's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GRPO in OpenRLHF
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-30 14:30:00" itemprop="dateCreated datePublished" datetime="2024-10-30T14:30:00+00:00">2024-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-11-10 08:30:36" itemprop="dateModified" datetime="2025-11-10T08:30:36+00:00">2025-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GRPO/" itemprop="url" rel="index"><span itemprop="name">GRPO</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GRPO/PPO/" itemprop="url" rel="index"><span itemprop="name">PPO</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GRPO/PPO/RL-Algorithms/" itemprop="url" rel="index"><span itemprop="name">RL-Algorithms</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="GRPO"><a href="#GRPO" class="headerlink" title="GRPO"></a>GRPO</h1><h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><ul>
<li><p>Trajectory Level Expectation</p>
<script type="math/tex; mode=display">
argmax(\pi_{\theta}) J_(\pi_{\theta)} = E_{\tau\sim \pi_{\theta}}[R(\tau)] = \sum_{\tau}R(\tau)P(\tau|\pi_{\theta}) \\</script></li>
<li><p>Derivate (REINFORCE: score function gradient estimation)</p>
<script type="math/tex; mode=display">
\nabla J_(\pi_{\theta)} = E_{\tau\sim \pi_{\theta}}[R(\tau)\nabla log(P({\tau|\pi_\theta}))]</script></li>
<li><p>Step Level Expectation</p>
<script type="math/tex; mode=display">
\nabla J_(\pi_{\theta)} = E_{\tau\sim \pi_{\theta}}[R(\tau)\nabla \sum_{t=0}^{t=T_n}log(\pi_\theta({a_t|s_t}))]</script></li>
<li><p>Policy Gradient</p>
</li>
</ul>
<script type="math/tex; mode=display">
\nabla J(\pi_{\theta}) = \frac{1}{N} \sum_{n=0}^{N-1} \sum_{t=0}^{T_{n} -1} R_{(T_n)}\nabla log \pi_{\theta}(a_{t}|s_{t})  \\</script><p><strong>value func</strong> of the whole trajectory, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="2.228ex" height="1.33ex" role="img" focusable="false" viewBox="0 -431 984.6 588.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g></g></svg></mjx-container> is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.699ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 751 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g></g></svg></mjx-container> of each action for each step</p>
<ul>
<li><h3 id="Implention-of-OpenRLHF"><a href="#Implention-of-OpenRLHF" class="headerlink" title="Implention of OpenRLHF"></a>Implention of OpenRLHF</h3><p><strong>Preliminary</strong>: the original objective func of policy gradient RL  is the expection of value func on all sampled trajectories: $ E<em>\tau\sim \pi</em>{\theta}  [R(\tau)] $, so the objective of GRPO is</p>
</li>
</ul>
<script type="math/tex; mode=display">
  J = (\frac{1}{G} \sum_{i=1}^{G} ) (\frac{1}{|O_i|} \sum_{t=1}^{|o_i|} ) \{ min [  r_{i,t}(\theta) Adv_{(i,t)} , clip (r_{i,t}(\theta), i-\epsilon, 1+\epsilon) Adv_{(i,t)}] - \beta(KL)\}</script><p>  Implention of OpenRLHF</p>
<p>  <strong>step1: compute $\pi<em>{\theta</em>{old} }$</strong>:[bs, seq_len]</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">experience_maker.py</span><br><span class="line"><span class="number">1.</span> rollout_samples.sequences: <span class="built_in">list</span>(output.prompt_token_ids) + <span class="built_in">list</span>(output.outputs[<span class="number">0</span>].token_ids)  <span class="comment"># (experience_maker.py: _generate_vllm)</span></span><br><span class="line"><span class="number">2.</span> output = <span class="variable language_">self</span>.model(sequences, attention_mask=foward_attention_mask, position_ids=position_ids)[<span class="string">"logits"</span>]  <span class="comment"># size:  [micro_bs, seq_len, voc_size] (actor.py: forward)</span></span><br><span class="line">log_probs = log_probs_from_logits(output[<span class="string">"logits"</span>], rolled_sequences, temperature=<span class="variable language_">self</span>.temperature) <span class="comment"># [micro_bs, seq_len], compute the log_softmax value of labeled token</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">logits = torch.tensor([[[1.0, 2.0, 3.0],  </span></span><br><span class="line"><span class="string">                        [0.5, 1.5, 2.5]]]) </span></span><br><span class="line"><span class="string">rolled_sequences = torch.tensor([[2, 1]])  # label</span></span><br><span class="line"><span class="string">logits_labels = torch.tensor([[3.0, 1.5]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">logsumexp = log(exp(1.0) + exp(2.0) + exp(3.0)) ≈ 3.407</span></span><br><span class="line"><span class="string">logsumexp = log(exp(0.5) + exp(1.5) + exp(2.5)) ≈ 2.907</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">log_probs = logits_labels - logsumexp_values</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">action_log_probs_list: <span class="built_in">len</span>(<span class="built_in">list</span>) = [bs]</span><br><span class="line"><span class="number">3.</span> base_action_log_probs_list (reference model)</span><br><span class="line"><span class="string">""""""</span><span class="string">"""""</span></span><br><span class="line"><span class="string">samples_list.action_log_probs： \theta_old</span></span><br><span class="line"><span class="string">samples_list.base_action_log_probs</span></span><br><span class="line"><span class="string">"""</span><span class="string">""""""</span><span class="string">""</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  <strong>Step2: compute Adv over <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="3.562ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 1574.4 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(485,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(783,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></svg></mjx-container></strong>,[bs, seq_len]</p>
<script type="math/tex; mode=display">
  R = Adv_{i} = \frac{r_i - mean(r_1, r_2, ...., r_G)}{std(r_1, r_2, ...., r_G)}</script>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">experience_maker.py</span><br><span class="line"><span class="number">1.</span> group_reward_std: group_reward_stds = (rewards.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>).repeat(<span class="number">1</span>, args.n_samples_per_prompt).reshape(-<span class="number">1</span>)[indices].split(exp_len)) <span class="comment"># [rollout_bs, n_samples_per_prompt]</span></span><br><span class="line"><span class="number">2.</span> adv</span><br><span class="line">rewards = (rewards - rewards.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)) / (rewards.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="number">1e-9</span>) <span class="comment"># [rollout_bs, n_samples]</span></span><br><span class="line">rewards = rewards.reshape(-<span class="number">1</span>)[indices].split(exp_len) <span class="comment"># flattened</span></span><br><span class="line">    <span class="keyword">for</span> experience, reward <span class="keyword">in</span> <span class="built_in">zip</span>(experiences, rewards):</span><br><span class="line">    <span class="comment"># rewards:  scalar-&gt;sequence. a)For each sample, reward is set on the index of 				last token of output sequence. b)kl_loss is computed token by token among ref and 		actor model</span></span><br><span class="line">            reward = compute_reward(</span><br><span class="line">                reward,</span><br><span class="line">                <span class="variable language_">self</span>.kl_ctl.value,</span><br><span class="line">                experience.kl,</span><br><span class="line">                action_mask=experience.action_mask,</span><br><span class="line">                reward_clip_range=args.reward_clip_range,</span><br><span class="line">            ) <span class="comment"># [seq_len]</span></span><br><span class="line">	<span class="comment"># reward is equal for all tokens ：tensor([[-0.7071, -0.7071, -0.7071,  ..., -0.7071, -0.7071, -0.7071]])</span></span><br><span class="line">                args.gamma = <span class="number">1.0</span></span><br><span class="line">                experience.returns = <span class="variable language_">self</span>.get_cumulative_returns(</span><br><span class="line">                    reward,</span><br><span class="line">                    experience.action_mask,</span><br><span class="line">                    args.gamma,</span><br><span class="line">                )</span><br><span class="line">                experience.advantages = deepcopy(experience.returns)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">f"Unkown advantage_estimator <span class="subst">{self.advantage_estimator}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  <strong>Step3: Importance sampling — off policy</strong></p>
<script type="math/tex; mode=display">
  r_{i,t}(\theta) = \frac{\pi_\theta}{\pi_{\theta_{old} } }</script>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">$\pi_{\theta}$ -&gt; train_batch, $\pi_{\theta_{old} }$ -&gt; rollout batch.  rollout batch &gt; train_batch, GRPO is off-policy.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  Use policy<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$\pi_{\theta_{old} }$  to approximate expection of $\pi_{\theta}$ which haven't been computed.</span><br><span class="line"></span><br><span class="line">**Step4: loss func**</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">loss.py</span><br><span class="line">def forward(</span><br><span class="line">        self,</span><br><span class="line">        log_probs: torch.Tensor,</span><br><span class="line">        old_log_probs: torch.Tensor,</span><br><span class="line">        advantages: torch.Tensor,</span><br><span class="line">        action_mask: Optional[torch.Tensor] = None,</span><br><span class="line">        rollout_log_probs: Optional[torch.Tensor] = None,</span><br><span class="line">    ) -&gt; torch.Tensor:</span><br><span class="line">        if self.policy_loss_type == "ppo":</span><br><span class="line">            log_ratio = log_probs - old_log_probs</span><br><span class="line">            ratio = log_ratio.exp()</span><br><span class="line">        elif self.policy_loss_type == "gspo":</span><br><span class="line">            # GSPO: https://arxiv.org/pdf/2507.18071</span><br><span class="line">            if self.enable_vllm_is_correction:</span><br><span class="line">                log_ratio = log_probs - rollout_log_probs</span><br><span class="line">            else:</span><br><span class="line">                log_ratio = log_probs - old_log_probs</span><br><span class="line">            ratio = (log_ratio * action_mask).sum(dim=-1) / action_mask.sum(dim=-1)</span><br><span class="line">            ratio = ratio.exp().unsqueeze(-1) * action_mask</span><br><span class="line">        else:</span><br><span class="line">            raise ValueError(f"Invalid policy loss type: {self.policy_loss_type}")</span><br><span class="line"></span><br><span class="line">        surr1 = ratio * advantages</span><br><span class="line">        surr2 = ratio.clamp(1 - self.clip_eps_low, 1 + self.clip_eps_high) * advantages</span><br><span class="line"></span><br><span class="line">        if self.dual_clip is None:</span><br><span class="line">            # Standard PPO</span><br><span class="line">            loss = -torch.min(surr1, surr2)</span><br><span class="line">        else:</span><br><span class="line">            # Standard PPO clipping</span><br><span class="line">            clip1 = torch.min(surr1, surr2)</span><br><span class="line">            # Dual-clip: additional lower bound for negative advantages</span><br><span class="line">            clip2 = torch.max(clip1, self.dual_clip * advantages)</span><br><span class="line">            # Apply dual-clip: use clip2 for negative advantages, clip1 for positive advantages</span><br><span class="line">            loss = -torch.where(advantages &lt; 0, clip2, clip1)</span><br><span class="line"></span><br><span class="line">        # Your Efficient RL Framework Secretly Brings You Off-Policy RL Training: https://fengyao.notion.site/off-policy-rl</span><br><span class="line">        vllm_kl = None</span><br><span class="line">        if self.enable_vllm_is_correction and self.policy_loss_type == "ppo":</span><br><span class="line">            vllm_is = torch.exp(old_log_probs - rollout_log_probs).clamp(max=self.vllm_is_truncated_threshold).detach()</span><br><span class="line">            loss = vllm_is * loss</span><br><span class="line">            vllm_kl = masked_mean(rollout_log_probs - old_log_probs, action_mask, dim=None)</span><br><span class="line"></span><br><span class="line">        loss = (</span><br><span class="line">            masked_mean(loss, action_mask, dim=None)</span><br><span class="line">            if self.token_level_loss</span><br><span class="line">            else masked_mean(loss, action_mask, dim=-1).mean()</span><br><span class="line">        )</span><br><span class="line">        clip_ratio = masked_mean(torch.lt(surr2, surr1).float(), action_mask, dim=None)</span><br><span class="line">        ppo_kl = masked_mean(-log_ratio.detach(), action_mask, dim=None)</span><br><span class="line">        return loss, clip_ratio, ppo_kl, vllm_kl</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/0205/10/30/RLHF-GRPO/" rel="prev" title="GRPO-Algorithm">
                  <i class="fa fa-angle-left"></i> GRPO-Algorithm
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">jiru</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
